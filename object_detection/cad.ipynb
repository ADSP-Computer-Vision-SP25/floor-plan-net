{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T04:47:53.467207Z",
     "start_time": "2025-05-16T04:47:49.891347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert_coco.py\n",
    "from ultralytics.data.converter import convert_coco\n",
    "\n",
    "# point this at your COCO JSON folder:\n",
    "convert_coco(\n",
    "    labels_dir=\"../data/cad/annotations\",  # contains your train.json & val.json\n",
    "    save_dir=\"../data/cad\",                # will create labels/train & labels/val\n",
    "    use_segments=True,                           # include mask polygons\n",
    "    cls91to80=False                              # our dataset only has 16 classes\n",
    ")\n"
   ],
   "id": "64f3aa0097e184d4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotations /home/ubuntu/floor-plan-net/data/cad/annotations/test-00-annotations.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3893/3893 [00:00<00:00, 7921.33it/s]\n",
      "Annotations /home/ubuntu/floor-plan-net/data/cad/annotations/train-00-annotations.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2903/2903 [00:00<00:00, 7626.46it/s]\n",
      "Annotations /home/ubuntu/floor-plan-net/data/cad/annotations/train-01-annotations.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4336/4336 [00:00<00:00, 8961.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO data converted successfully.\n",
      "Results saved to /home/ubuntu/floor-plan-net/data/cad3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T16:25:32.661194Z",
     "start_time": "2025-05-16T16:25:21.763381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# load your segmentation-capable checkpoint\n",
    "model = YOLO(\"models/yolo11l-seg.pt\")\n",
    "\n",
    "# train\n",
    "model.train(\n",
    "    data=\"data/cad/cad.yaml\",\n",
    "    epochs=20,\n",
    "    batch=16,\n",
    "    imgsz=640,\n",
    "    device=0,       # GPU 0 (or \"cpu\")\n",
    "    lr0=2e-3,\n",
    "    freeze=0,\n",
    "    name=\"cad-seg-run\"\n",
    ")\n"
   ],
   "id": "bb24cac0bae2140",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.136 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.127 ðŸš€ Python-3.12.3 torch-2.7.0+cu128 CUDA:0 (NVIDIA GeForce RTX 5070 Ti, 16303MiB)\n",
      "\u001B[34m\u001B[1mengine/trainer: \u001B[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data/cad/cad.yaml, degrees=0.0, deterministic=True, device=cuda:0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=0, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.002, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11l-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=cad-seg-run17, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/segment/cad-seg-run17, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=31\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   3741133  ultralytics.nn.modules.head.Segment          [31, 32, 256, [256, 512, 512]]\n",
      "YOLO11l-seg summary: 379 layers, 27,640,589 parameters, 27,640,573 gradients, 142.8 GFLOPs\n",
      "\n",
      "Transferred 1071/1077 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed âœ…\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5052.6Â±3595.2 MB/s, size: 44.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning /home/ubuntu/floor-plan-net/data/cad/labels/train-01.cache... 4336 images, 2065 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6401/6401 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 295.8Â±372.5 MB/s, size: 145.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mScanning /home/ubuntu/floor-plan-net/data/cad/labels/train-00.cache... 2903 images, 857 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/segment/cad-seg-run17/labels.jpg... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.002' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.000286, momentum=0.9) with parameter groups 176 weight(decay=0.0), 187 weight(decay=0.0005), 186 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001B[1mruns/segment/cad-seg-run17\u001B[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      12.3G      1.929      3.415      5.293      1.452        144        640:   0%|          | 2/401 [00:01<04:35,  1.45it/s]\n",
      "Exception in thread Thread-48 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m model = YOLO(\u001B[33m\"\u001B[39m\u001B[33myolo11l-seg.pt\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# train\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdata/cad/cad.yaml\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimgsz\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m640\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m       \u001B[49m\u001B[38;5;66;43;03m# GPU 0 (or \"cpu\")\u001B[39;49;00m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr0\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2e-3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfreeze\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcad-seg-run\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     16\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/pytorch/lib/python3.12/site-packages/ultralytics/engine/model.py:793\u001B[39m, in \u001B[36mModel.train\u001B[39m\u001B[34m(self, trainer, **kwargs)\u001B[39m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28mself\u001B[39m.model = \u001B[38;5;28mself\u001B[39m.trainer.model\n\u001B[32m    792\u001B[39m \u001B[38;5;28mself\u001B[39m.trainer.hub_session = \u001B[38;5;28mself\u001B[39m.session  \u001B[38;5;66;03m# attach optional HUB session\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m793\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    794\u001B[39m \u001B[38;5;66;03m# Update model and cfg after training\u001B[39;00m\n\u001B[32m    795\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;129;01min\u001B[39;00m {-\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m}:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/pytorch/lib/python3.12/site-packages/ultralytics/engine/trainer.py:211\u001B[39m, in \u001B[36mBaseTrainer.train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    208\u001B[39m         ddp_cleanup(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mstr\u001B[39m(file))\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m211\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/pytorch/lib/python3.12/site-packages/ultralytics/engine/trainer.py:385\u001B[39m, in \u001B[36mBaseTrainer._do_train\u001B[39m\u001B[34m(self, world_size)\u001B[39m\n\u001B[32m    383\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m autocast(\u001B[38;5;28mself\u001B[39m.amp):\n\u001B[32m    384\u001B[39m     batch = \u001B[38;5;28mself\u001B[39m.preprocess_batch(batch)\n\u001B[32m--> \u001B[39m\u001B[32m385\u001B[39m     loss, \u001B[38;5;28mself\u001B[39m.loss_items = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    386\u001B[39m     \u001B[38;5;28mself\u001B[39m.loss = loss.sum()\n\u001B[32m    387\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m RANK != -\u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/pytorch/lib/python3.12/site-packages/ultralytics/nn/tasks.py:114\u001B[39m, in \u001B[36mBaseModel.forward\u001B[39m\u001B[34m(self, x, *args, **kwargs)\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    101\u001B[39m \u001B[33;03mPerform forward pass of the model for either training or inference.\u001B[39;00m\n\u001B[32m    102\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    111\u001B[39m \u001B[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001B[39;00m\n\u001B[32m    112\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mdict\u001B[39m):  \u001B[38;5;66;03m# for cases of training and validating while training.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.predict(x, *args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/pytorch/lib/python3.12/site-packages/ultralytics/nn/tasks.py:301\u001B[39m, in \u001B[36mBaseModel.loss\u001B[39m\u001B[34m(self, batch, preds)\u001B[39m\n\u001B[32m    298\u001B[39m     \u001B[38;5;28mself\u001B[39m.criterion = \u001B[38;5;28mself\u001B[39m.init_criterion()\n\u001B[32m    300\u001B[39m preds = \u001B[38;5;28mself\u001B[39m.forward(batch[\u001B[33m\"\u001B[39m\u001B[33mimg\u001B[39m\u001B[33m\"\u001B[39m]) \u001B[38;5;28;01mif\u001B[39;00m preds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m preds\n\u001B[32m--> \u001B[39m\u001B[32m301\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    self.run()\n",
      "  File \"/home/ubuntu/.virtualenvs/pytorch/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/.virtualenvs/pytorch/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 61, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/ubuntu/.virtualenvs/pytorch/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.virtualenvs/pytorch/lib/python3.12/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
      "    c = SocketClient(address)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:52:27.093672Z",
     "start_time": "2025-05-16T05:52:26.643981Z"
    }
   },
   "cell_type": "code",
   "source": "preds = model.predict(source=\"0001-0040.png\", save=True)\n",
   "id": "ce00862a0c76484b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ubuntu/floor-plan-net/0001-0040.png: 640x640 6 single doors, 7 windows, 11 opening symbols, 4 sofas, 2 beds, 4 chairs, 3 tables, 2 tv cabinets, 1 wardrobe, 4 cabinets, 1 refrigerator, 5 airconditioners, 1 gas stove, 2 sinks, 1 washing machine, 1 toilet, 10.3ms\n",
      "Speed: 6.9ms preprocess, 10.3ms inference, 18.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001B[1mruns/segment/cad-seg-run162\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:53:58.573318Z",
     "start_time": "2025-05-16T05:53:58.441879Z"
    }
   },
   "cell_type": "code",
   "source": "model.export(\"yolo11l-seg-cad.pt\")",
   "id": "4f861272b5a04f82",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Model.export() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43myolo11l-seg-cad.pt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: Model.export() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T15:32:03.403087Z",
     "start_time": "2025-05-21T15:32:01.637552Z"
    }
   },
   "cell_type": "code",
   "source": "from ultralytics import YOLO",
   "id": "ebcd5528ad14aa8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T15:32:04.859739Z",
     "start_time": "2025-05-21T15:32:04.782718Z"
    }
   },
   "cell_type": "code",
   "source": "loaded_model = YOLO(\"models/yolo11l-seg-cad.pt\")",
   "id": "710643673372ddc9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T15:32:08.575235Z",
     "start_time": "2025-05-21T15:32:05.976259Z"
    }
   },
   "cell_type": "code",
   "source": "preds = loaded_model.predict(source=\"capri_floorplan-1.jpg\", save=True)\n",
   "id": "1aa82d1d0d0c7d24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ubuntu/floor-plan-net/capri_floorplan-1.jpg: 640x640 5 single doors, 1 double door, 2 gas stoves, 1 sink, 3 stairs, 2 elevators, 10.1ms\n",
      "Speed: 5.1ms preprocess, 10.1ms inference, 230.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001B[1mruns/segment/predict7\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:06:50.655901Z",
     "start_time": "2025-05-21T16:06:50.383475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "results = loaded_model('data/cad/images/train-01/0126-0009.png', save=True)  # or a list of paths\n",
    "\n",
    "# results is a list of Results objectsâ€”loop if you passed multiple images\n",
    "det_list = []\n",
    "for res in results:\n",
    "    for box in res.boxes:  # .boxes holds your detections\n",
    "        # Access the first item of each coordinate using indexing [0]\n",
    "        det_list.append({\n",
    "            'class_id': int(box.cls[0]),  # Add [0] to get first element\n",
    "            'x': float(box.xywh[0][0]),  # center-x\n",
    "            'y': float(box.xywh[0][1]),  # center-y\n",
    "            'w': float(box.xywh[0][2]),  # width\n",
    "            'h': float(box.xywh[0][3]),  # height\n",
    "            'confidence': float(box.conf[0])  # Add [0] to get first element\n",
    "        })\n",
    "\n",
    "# write to JSON\n",
    "with open('detections.json', 'w') as f:\n",
    "    json.dump(det_list, f, indent=2)"
   ],
   "id": "7f2b20799b08c327",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ubuntu/floor-plan-net/data/cad/images/train-01/0126-0009.png: 640x640 4 single doors, 1 window, 3 sofas, 2 beds, 2 chairs, 2 tables, 1 tv cabinet, 3 wardrobes, 6 cabinets, 2 sinks, 1 bath, 1 washing machine, 2 toilets, 1 elevator, 74.2ms\n",
      "Speed: 4.6ms preprocess, 74.2ms inference, 39.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001B[1mruns/segment/predict7\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
